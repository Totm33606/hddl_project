{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Projet SSL for Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms.functional import rgb_to_grayscale\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score\n",
    "import PIL\n",
    "from PIL import Image\n",
    "import random\n",
    "import opendatasets as od\n",
    "import os\n",
    "from time import time\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to store the datasets\n",
    "os.makedirs(\"datasets\", exist_ok=True)\n",
    "\n",
    "mvtec_ad_url = \"https://www.kaggle.com/datasets/ipythonx/mvtec-ad\"\n",
    "auto_vi_url = \"https://zenodo.org/api/records/10459003/files-archive\"\n",
    "\n",
    "od.download(mvtec_ad_url, data_dir=\"datasets/MVTec_AD\")\n",
    "\n",
    "od.download(auto_vi_url, data_dir=\"datasets/AutoVI\")\n",
    "!unzip datasets/AutoVI/10459003.zip -d datasets/AutoVI\n",
    "!unzip datasets/AutoVI/engine_wiring.zip -d datasets/AutoVI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "TRANSFORM = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "MVTEC_PATH = 'datasets/MVTec_AD/mvtec-ad'\n",
    "AUTOVI_PATH = 'datasets/AutoVI/engine_wiring'\n",
    "CATEGORIES_MVTEC = ['bottle', 'hazelnut', 'capsule', 'toothbrush']\n",
    "CATEGORIES_AUTOVI = ['blue_hoop', 'cardboard', 'fastening', 'obstruction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement et prétraitement des images de MVTec AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mvtec_images(category, split='train'):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    split_path = os.path.join(MVTEC_PATH, category, split)\n",
    "    \n",
    "    if split == 'train':\n",
    "        good_path = os.path.join(split_path, 'good')\n",
    "        for img_name in os.listdir(good_path):\n",
    "            img_path = os.path.join(good_path, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = TRANSFORM(img)\n",
    "            images.append(img)\n",
    "            labels.append(0)\n",
    "    \n",
    "    else:\n",
    "        for anomaly_type in os.listdir(split_path):\n",
    "            anomaly_path = os.path.join(split_path, anomaly_type)\n",
    "            for img_name in os.listdir(anomaly_path):\n",
    "                img_path = os.path.join(anomaly_path, img_name)\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img = TRANSFORM(img)\n",
    "                images.append(img)\n",
    "                \n",
    "                if anomaly_type == 'good':\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "\n",
    "    return torch.stack(images), torch.tensor(labels)\n",
    "\n",
    "train_images, train_labels = [], []\n",
    "test_images, test_labels = [], []\n",
    "\n",
    "for category in CATEGORIES_MVTEC:\n",
    "    imgs, lbls = load_mvtec_images(category, split='train')\n",
    "    train_images.append(imgs)\n",
    "    train_labels.append(lbls)\n",
    "    \n",
    "    imgs, lbls = load_mvtec_images(category, split='test')\n",
    "    test_images.append(imgs)\n",
    "    test_labels.append(lbls)\n",
    "\n",
    "train_images = torch.cat(train_images)\n",
    "train_labels = torch.cat(train_labels)\n",
    "test_images = torch.cat(test_images)\n",
    "test_labels = torch.cat(test_labels)\n",
    "\n",
    "train_dataloader_MVTec = DataLoader(list(zip(train_images, train_labels)), batch_size=16, shuffle=True)\n",
    "test_dataloader_MVTec = DataLoader(list(zip(test_images, test_labels)), batch_size=16, shuffle=False)\n",
    "\n",
    "print(f\"Loaded {len(train_images)} training images and {len(test_images)} test images from MVTec AD.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement et prétraitement des images de AutoVI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images from a specific split (train or test)\n",
    "def load_autovi_images(split='train'):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Define the path based on the split (train/test)\n",
    "    split_path = os.path.join(AUTOVI_PATH, split)\n",
    "    \n",
    "    # List all subfolders (categories)\n",
    "    categories = os.listdir(split_path)\n",
    "    \n",
    "    for category in categories:\n",
    "        category_path = os.path.join(split_path, category)\n",
    "        for img_name in os.listdir(category_path):\n",
    "            img_path = os.path.join(category_path, img_name)\n",
    "            img = Image.open(img_path).convert('RGB')  \n",
    "            img = TRANSFORM(img)  \n",
    "            images.append(img)\n",
    "            \n",
    "            # Use the index of the category as a label (you can adjust this if needed)\n",
    "            labels.append(categories.index(category))\n",
    "    \n",
    "    return torch.stack(images), torch.tensor(labels)  \n",
    "\n",
    "# Load AutoVI dataset images (train and test)\n",
    "train_images, train_labels = load_autovi_images(split='train')\n",
    "test_images, test_labels = load_autovi_images(split='test')\n",
    "\n",
    "# Create DataLoaders for AutoVI train and test sets\n",
    "train_dataloader_auto = DataLoader(list(zip(train_images, train_labels)), batch_size=16, shuffle=True)\n",
    "test_dataloader_auto = DataLoader(list(zip(test_images, test_labels)), batch_size=16, shuffle=False)\n",
    "\n",
    "# Check loaded data\n",
    "print(f\"Loaded {len(train_images)} training images and {len(test_images)} test images from AutoVI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    denormalized_tensor = tensor * 0.5 + 0.5\n",
    "    return denormalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random__MVTec(category_list, dataset_name, dataset_path, transform, split='test'):\n",
    "    fig, axs = plt.subplots(len(category_list), 2, figsize=(10, 5 * len(category_list)))  \n",
    "    fig.suptitle(f'Random Images from {dataset_name} (Good vs Anomalies)', fontsize=16)\n",
    "    \n",
    "    for i, category in enumerate(category_list):\n",
    "        # Path for the test split\n",
    "        split_path = os.path.join(dataset_path, category, split)\n",
    "        \n",
    "        # Load a random \"good\" image\n",
    "        good_path = os.path.join(split_path, 'good')\n",
    "        good_image = random.choice(os.listdir(good_path))\n",
    "        good_image_path = os.path.join(good_path, good_image)\n",
    "        img_good = Image.open(good_image_path).convert('RGB')\n",
    "        img_good = transform(img_good)\n",
    "        \n",
    "        # Display the \"good\" image\n",
    "        img_good = denormalize(img_good)\n",
    "        axs[i, 0].imshow(np.clip(img_good.permute(1, 2, 0).numpy(), 0, 1))  # Convert to NumPy and clip\n",
    "        axs[i, 0].set_title(f\"{category} (Good)\")\n",
    "        axs[i, 0].axis('off')\n",
    "        \n",
    "        # Load a random anomalous image (choose a random anomaly type)\n",
    "        anomalies = [d for d in os.listdir(split_path) if d != 'good']\n",
    "        anomaly_type = random.choice(anomalies)\n",
    "        anomaly_path = os.path.join(split_path, anomaly_type)\n",
    "        anomaly_image = random.choice(os.listdir(anomaly_path))\n",
    "        anomaly_image_path = os.path.join(anomaly_path, anomaly_image)\n",
    "        img_anomalous = Image.open(anomaly_image_path).convert('RGB')\n",
    "        img_anomalous = transform(img_anomalous)\n",
    "        \n",
    "        # Display the anomalous image\n",
    "        img_anomalous = denormalize(img_anomalous)\n",
    "        axs[i, 1].imshow(np.clip(img_anomalous.permute(1, 2, 0).numpy(), 0, 1))  # Convert to NumPy and clip\n",
    "        axs[i, 1].set_title(f\"{category} (Anomaly: {anomaly_type})\")\n",
    "        axs[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Example usage for MVTec AD dataset\n",
    "display_random__MVTec(CATEGORIES_MVTEC, \"MVTec AD\", MVTEC_PATH, TRANSFORM, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_random_autovi(category_list, dataset_name, dataset_path, transform, split='test'):\n",
    "    fig, axs = plt.subplots(1, len(category_list), figsize=(16, 8))  # Create subplot (1 row, 1 column per category)\n",
    "    fig.suptitle(f'Random Images from {dataset_name} (Anomalies)', fontsize=16)\n",
    "    \n",
    "    for i, category in enumerate(category_list):\n",
    "        # Path for the test split\n",
    "        split_path = os.path.join(dataset_path, split, category)\n",
    "        \n",
    "        # Load a random anomalous image\n",
    "        anomaly_image = random.choice(os.listdir(split_path))\n",
    "        anomaly_image_path = os.path.join(split_path, anomaly_image)\n",
    "        img_anomalous = Image.open(anomaly_image_path).convert('RGB')\n",
    "        img_anomalous = transform(img_anomalous)\n",
    "        \n",
    "        # Display the anomalous image\n",
    "        img_anomalous = denormalize(img_anomalous)\n",
    "        axs[i].imshow(np.clip(img_anomalous.permute(1, 2, 0).numpy(), 0, 1))  # Rearrange dimensions for display\n",
    "        axs[i].set_title(f\"{category} (Anomaly)\")\n",
    "        axs[i].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage for AutoVI dataset\n",
    "display_random_autovi(CATEGORIES_AUTOVI, \"AutoVI\", AUTOVI_PATH, TRANSFORM, split='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masked AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim,in_channels):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encode = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, latent_dim, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.encode(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, out_channels):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, kernel_size=4, stride=2, padding=1), \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),  \n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, out_channels, kernel_size=4, stride=2, padding=1), \n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Masked Autoencoder model\n",
    "class MaskedAutoencoder(nn.Module):\n",
    "    def __init__(self, mask_ratio, hidden_dim=1024):\n",
    "        super(MaskedAutoencoder, self).__init__()\n",
    "        self.encoder = Encoder(latent_dim=hidden_dim, in_channels=3)  # Input grayscale\n",
    "        self.decoder = Decoder(latent_dim=hidden_dim, out_channels=3)\n",
    "        self.mask_ratio = mask_ratio\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        masked_x = self.apply_mask(x)\n",
    "        encoded = self.encoder(masked_x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded.view(batch_size, channels, height, width)\n",
    "\n",
    "    def apply_mask(self, x):\n",
    "        x_masked = x.clone()\n",
    "        mask = torch.rand_like(x[:, 0, :, :]) < self.mask_ratio\n",
    "        mask = mask.unsqueeze(1).repeat(1, x.size(1), 1, 1)  \n",
    "        x_masked[mask] = 0\n",
    "        return x_masked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the model\n",
    "def train_model(model, dataloader, epochs, lr=0.001):\n",
    "    criterion = nn.MSELoss()  # Mean Squared Error for reconstruction\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for images, _ in tqdm(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Function to evaluate the model and calculate ROC-AUC\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss(reduction='none')  # Use reduction='none' to compute loss per pixel\n",
    "    losses = []\n",
    "    labels = []\n",
    "    seuil = 0.5\n",
    "    with torch.no_grad():\n",
    "        for images, lbls in tqdm(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images).mean(dim=[1, 2, 3])  # Mean loss over pixels\n",
    "            losses.extend(loss.cpu().numpy())\n",
    "            binary_labels = [1 if label != 0 else 0 for label in lbls]\n",
    "            labels.extend(binary_labels)\n",
    "            \n",
    "    # Appliquer le seuil pour classifier les images\n",
    "    predicted_labels = [1 if loss > seuil else 0 for loss in losses]\n",
    "    fpr, tpr, thresholds = roc_curve(labels, losses)\n",
    "\n",
    "    accuracy = accuracy_score(labels, predicted_labels)\n",
    "    print(f\"Accuracy with threshold {seuil}: {accuracy:.4f}\")\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Masked Autoencoder')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_dataloader_MVTec and test_dataloader_MVTec are defined\n",
    "# Initialize and train the model\n",
    "mae_model_MV = MaskedAutoencoder(mask_ratio = 1/5).to(DEVICE)\n",
    "train_model(mae_model_MV, train_dataloader_MVTec, epochs=5)\n",
    "torch.save(mae_model_MV.state_dict(), 'Masked_Autoencoder_MVTec_weights.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "roc_auc = evaluate_model(mae_model_MV, test_dataloader_MVTec)\n",
    "print(f\"AUROC Score: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming train_dataloader_MVTec and test_dataloader_MVTec are defined\n",
    "# Initialize and train the model\n",
    "mae_model_auto = MaskedAutoencoder(mask_ratio = 1/5).to(DEVICE)\n",
    "train_model(mae_model_auto, train_dataloader_auto, epochs=5)\n",
    "torch.save(mae_model_auto.state_dict(), 'Masked_Autoencoder_AUTO_weights.pth')\n",
    "\n",
    "# Evaluate the model\n",
    "roc_aucs = evaluate_model(mae_model_auto, test_dataloader_auto)\n",
    "print(f\"AUROC Score: {roc_aucs:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContrastiveModel, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=4, stride=1)\n",
    "        )\n",
    "        self.fc = nn.Linear(64 * 27 * 27, 64)  # Ajusté en fonction de la sortie des convolutions\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(x.size(0), -1)  # Aplatir les données\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, z1, z2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(z1, z2)\n",
    "        loss = torch.mean((1 - label) * torch.pow(euclidean_distance, 2) + \n",
    "                          (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_contrastive_pairs(batch_images, batch_labels):\n",
    "    pairs = []\n",
    "    pair_labels = []\n",
    "    \n",
    "    batch_size = len(batch_images)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Générer une paire positive (même classe)\n",
    "        positive_indices = [j for j in range(batch_size) if batch_labels[i] == batch_labels[j]]\n",
    "        if len(positive_indices) > 1:  # Vérifier qu'il y a au moins une autre image de la même classe\n",
    "            positive_index = random.choice([j for j in positive_indices if j != i])\n",
    "            pairs.append((batch_images[i], batch_images[positive_index]))\n",
    "            pair_labels.append(1)  # Label 1 pour paire positive\n",
    "        \n",
    "        # Générer une paire négative (classe différente)\n",
    "        negative_indices = [j for j in range(batch_size) if batch_labels[i] != batch_labels[j]]\n",
    "        if len(negative_indices) > 0:  # Vérifier qu'il y a au moins une image d'une classe différente\n",
    "            negative_index = random.choice(negative_indices)\n",
    "            pairs.append((batch_images[i], batch_images[negative_index]))\n",
    "            pair_labels.append(0)  # Label 0 pour paire négative\n",
    "\n",
    "    # Convertir en tenseurs\n",
    "    if pairs:  # Vérifier si des paires ont été générées\n",
    "        pair1, pair2 = zip(*pairs)\n",
    "        return (torch.stack(pair1), torch.stack(pair2)), torch.tensor(pair_labels)\n",
    "    else:\n",
    "        raise ValueError(\"Aucune paire n'a pu être générée dans ce batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'entraînement pour le modèle contrastif\n",
    "def train_contrastive_model(model, dataloader, epochs, lr=0.001, weight_decay=1e-4, patience=2):\n",
    "    criterion = nn.CosineEmbeddingLoss()  # Utilise la perte de similarité cosinus\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        for batch_images, batch_labels in tqdm(dataloader):\n",
    "            batch_images = batch_images.to(DEVICE)\n",
    "            batch_labels = batch_labels.to(DEVICE)\n",
    "\n",
    "            # Générer des paires contrastives\n",
    "            pairs, pair_labels = generate_contrastive_pairs(batch_images, batch_labels)\n",
    "\n",
    "            # Obtenir les représentations latentes des deux images dans la paire\n",
    "            z1 = model(pairs[0].to(DEVICE))  # Appliquer .to(device) à chaque image\n",
    "            z2 = model(pairs[1].to(DEVICE))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculer la perte contrastive (CosineEmbeddingLoss)\n",
    "            loss = criterion(z1, z2, pair_labels.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * batch_images.size(0)\n",
    "        avg_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Early Stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_contrastive_model(model, dataloader):\n",
    "    model.eval()\n",
    "    distances = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch_images, batch_labels in tqdm(dataloader):\n",
    "            pairs, pair_labels = generate_contrastive_pairs(batch_images, batch_labels)\n",
    "            z1 = model(pairs[0].to(DEVICE))\n",
    "            z2 = model(pairs[1].to(DEVICE))\n",
    "            euclidean_distance = nn.functional.pairwise_distance(z1, z2)\n",
    "            distances.extend(euclidean_distance.cpu().numpy())\n",
    "            labels.extend(pair_labels.cpu().numpy())\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(labels, distances)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Contrastive Model')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrastive_model = ContrastiveModel().to(DEVICE)\n",
    "train_contrastive_model(contrastive_model, train_dataloader_MVTec, epochs=2)\n",
    "\n",
    "roc_auc_score = evaluate_contrastive_model(contrastive_model, test_dataloader_MVTec)\n",
    "print(f\"AUROC Score: {roc_auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colorizing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Other way to load data --> Choose together which one to keep\n",
    "## This one allows you to train on data with no anomalies\n",
    "## Keep in mind to evaluate the model with a test loader which contains anomalies\n",
    "class MVTECDataset(Dataset):\n",
    "    def __init__(self, data_dir=\"datasets/MVTec_AD/mvtec-ad\", split=\"train\", transform=TRANSFORM):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.image_paths = []\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        for folder in self.data_dir.iterdir():\n",
    "            if folder.is_dir():\n",
    "                folder_path = folder / self.split / \"good\"\n",
    "                self.image_paths.extend(folder_path.rglob(\"*.png\"))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # 0 because no anomaly\n",
    "        label = 0\n",
    "        \n",
    "        return image, label\n",
    "        \n",
    "train_dataset_MVTEC = MVTECDataset()\n",
    "print(f\"Train dataset size: {len(train_dataset_MVTEC)}\")\n",
    "\n",
    "#test_dataset_MVTEC = MVTECDataset(split=\"test\")\n",
    "#print(f\"Test dataset size: {len(test_dataset_MVTEC)}\")\n",
    "\n",
    "train_loader_MVTEC = DataLoader(train_dataset_MVTEC, batch_size=16, shuffle=True, num_workers=14, pin_memory=True)\n",
    "#test_loader_MVTEC = DataLoader(test_dataset_MVTEC, batch_size=1, shuffle=False, num_workers=14, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose the right number of workers ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the right number of workers\n",
    "dataset = MVTECDataset()\n",
    "for num_workers in range(2, mp.cpu_count(), 2):  \n",
    "    train_loader = DataLoader(dataset, shuffle=False, num_workers=num_workers, batch_size=16, pin_memory=True)\n",
    "    start = time()\n",
    "    for epoch in range(1, 3):\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            pass\n",
    "    end = time()\n",
    "    print(\"Finish with: {} seconds, num_workers={}\".format(end - start, num_workers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets choose a Unet to perform this task\n",
    "def double_conv(in_channels, out_channels):\n",
    "    # returns a block composed of two Convolution layers with ReLU activation function\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU()\n",
    "    )   \n",
    "\n",
    "class DownSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = double_conv(in_channels, out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_skip = self.conv_block(x) # output after convolutions to be used for skip connection\n",
    "        out = self.maxpool(x_skip)  # output after maxpooling for downsampling\n",
    "\n",
    "        return out, x_skip\n",
    "\n",
    "class UpSampleBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_block = double_conv(in_channels, out_channels)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "    def forward(self, x, x_skip):\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_skip], dim=1) # concatenate the upsampled output with skip connection\n",
    "        x = self.conv_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define downsampling blocks\n",
    "        self.downsample_block_1 = DownSampleBlock(1, 32)\n",
    "        self.downsample_block_2 = DownSampleBlock(32, 64)\n",
    "        self.downsample_block_3 = DownSampleBlock(64, 128)\n",
    "        self.middle_conv_block = double_conv(128, 256)  # Middle convolution block\n",
    "\n",
    "        # Define upsampling blocks\n",
    "        self.upsample_block_3 = UpSampleBlock(256 + 128, 128)\n",
    "        self.upsample_block_2 = UpSampleBlock(128 + 64, 64)\n",
    "        self.upsample_block_1 = UpSampleBlock(64 + 32, 32)\n",
    "\n",
    "        # Final output layer\n",
    "        self.last_conv = nn.Conv2d(32, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convert to gray image\n",
    "        x = rgb_to_grayscale(x, num_output_channels=1)\n",
    "        \n",
    "        # Downsampling path\n",
    "        x, x_skip1 = self.downsample_block_1(x)\n",
    "        x, x_skip2 = self.downsample_block_2(x)\n",
    "        x, x_skip3 = self.downsample_block_3(x)\n",
    "\n",
    "        # Middle block\n",
    "        x = self.middle_conv_block(x)\n",
    "\n",
    "        # Upsampling path with skip connections\n",
    "        x = self.upsample_block_3(x, x_skip3)\n",
    "        x = self.upsample_block_2(x, x_skip2)\n",
    "        x = self.upsample_block_1(x, x_skip1)\n",
    "\n",
    "        # Final output layer with sigmoid activation\n",
    "        out = torch.sigmoid(self.last_conv(x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet().to(DEVICE)\n",
    "train_model(model=unet, dataloader=train_loader_MVTEC, epochs=5)\n",
    "torch.save(unet.state_dict(), 'Colorize_UNet_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = evaluate_model(unet, test_dataloader_MVTec)\n",
    "images, _ = next(iter(test_dataloader_MVTec))\n",
    "images = images.to(DEVICE)\n",
    "outputs = unet(images)\n",
    "\n",
    "_, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,6))\n",
    "\n",
    "# Original image\n",
    "images = denormalize(images)\n",
    "axs[0].imshow(np.clip(images[0].permute(1, 2, 0).detach().cpu().numpy(), 0, 1))\n",
    "axs[0].set_title('Original')\n",
    "\n",
    "# Reconstructed image\n",
    "outputs = denormalize(outputs)\n",
    "axs[1].imshow(np.clip(outputs[0].permute(1, 2, 0).detach().cpu().numpy(), 0, 1))\n",
    "axs[1].set_title('Reconstructed')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## Note this doesn't work very well because of our dataset.. \n",
    "## Another idea to colorize images is described in this paper:\n",
    "## https://ceur-ws.org/Vol-2485/paper47.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
