{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7dbb18c-3fdd-4eb1-affe-a624a9e694b3",
   "metadata": {},
   "source": [
    "# Mini-project n° 1 - Who painted this picture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854d5dc-ff44-4e71-8b50-aae217cd7469",
   "metadata": {},
   "source": [
    " Le but de ce premier mini-projet est de construire et d'entraîner un réseau dans l'optique de répondre au\n",
    " challenge artchallenge.ru– \"Qui a peint ce tableau ?\".\n",
    "\n",
    " Pour cela, nous nous reférerons au jeu de données \"Art Challenge\" - https://plmlab.math.cnrs.fr/chevallier-teaching/datasets/art-challenge\n",
    " conteneant :\n",
    "- Un fichier `artists.csv` contenant la liste des artistes ainsi que leur année de naissance et de mort, une mini biographie, leur style de peinture de prédilection, le nombre d’œuvres étudiées dans ce dataset, et un lien vers leur page wikipédia.\n",
    "- Un dossier `images_lq` contenant un jeu de données de tableaux en basse qualité, nommés par auteurs.\n",
    "- Un dossier `images_hq` contenant le même jeu de données de tableau mais en haute qualité.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e098807-84da-457c-adc6-406e000b8eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
    "from torchvision import datasets, transforms, models\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e31476-d2eb-457e-9b20-4e07681d7e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6ad675-8a69-45b1-95db-46fa7e7f0ba7",
   "metadata": {},
   "source": [
    "## Loading datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54dc941-d0dc-48c9-b12d-9a1b019530c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./art-challenge/\"\n",
    "if not os.path.isdir(data_path) or not os.listdir(data_path):\n",
    "    !git clone https://plmlab.math.cnrs.fr/chevallier-teaching/datasets/art-challenge.git\n",
    "else:\n",
    "    print(\"Data already downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81537d54-7e49-4131-af3c-b3972ff073c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists = pd.read_csv(data_path + \"artists.csv\")\n",
    "artists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8fe714-bde6-4f5b-9bd3-5f53aaae1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists_filtered = artists.loc[artists['paintings'] > 85, 'name']\n",
    "artists_list = artists_filtered.tolist()\n",
    "artists_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a05a47-0ec9-4b44-9e9a-2106d28bc0df",
   "metadata": {},
   "source": [
    "## Analyse exploratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e33241-6fc1-41ef-9f80-f225d896a1dc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyse descriptive des artistes et des styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c10d36-02ce-42fb-8e21-73931058d7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre total d'artistes :\", artists['name'].nunique())\n",
    "print(\"Nombre total de styles :\", artists['genre'].nunique())\n",
    "\n",
    "artists['paintings'].plot(kind='hist', bins=40, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution des œuvres par artiste\")\n",
    "plt.xlabel(\"Nombre d'œuvres\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfadc6ee-9bad-49a5-bce6-997133d05a39",
   "metadata": {},
   "source": [
    "En dessous, on considère seulement les artistes avec moins de 200 oeuvres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210d939-ce8a-42c7-8727-89dc82fa3808",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre total d'artistes :\", artists['name'].nunique())\n",
    "print(\"Nombre total de styles :\", artists['genre'].nunique())\n",
    "\n",
    "artists['paintings'].plot(kind='hist', bins=80, color='skyblue', edgecolor='black')\n",
    "plt.title(\"Distribution des œuvres par artiste\")\n",
    "plt.xlabel(\"Nombre d'œuvres\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.xlim(0, 200)\n",
    "plt.show()\n",
    "\n",
    "styles = artists['genre'].str.split(',').explode()\n",
    "style_counts = styles.value_counts()\n",
    "style_counts.plot(kind='bar', color='lightgreen', edgecolor='black')\n",
    "plt.title(\"Nombre d'artistes par style de peinture\")\n",
    "plt.xlabel(\"Style de peinture\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fbd6c4-06e1-4aca-a26a-bfa4c396b84e",
   "metadata": {},
   "source": [
    "On voit que la majorité des artistes ont un nombre d'oeuvre compris entre 100 et 200. Nous allons zoomer sur cette zone, de sorte à ce que nous puissions mieux observer les nombres d'oeuvres par artistes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daaf429-e913-499f-86dc-62ed845aae58",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Nombre d'oeuvres par artistes et par genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840eecf3-bff9-4d3d-86e5-5860519e39b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_max = artists.loc[artists['paintings'].idxmax()]\n",
    "print(\"Artiste avec le plus d'œuvres :\")\n",
    "print(\"Nom :\", artist_max['name'])\n",
    "print(\"Nombre d'œuvres :\", artist_max['paintings'])\n",
    "print(\"Genre :\", artist_max['genre'])\n",
    "\n",
    "artist_min = artists.loc[artists['paintings'].idxmin()]\n",
    "print(\"\\nArtiste avec le moins d'œuvres :\")\n",
    "print(\"Nom :\", artist_min['name'])\n",
    "print(\"Nombre d'œuvres :\", artist_min['paintings'])\n",
    "print(\"Genre :\", artist_min['genre'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ba6aa-c246-4c05-8705-86b6ab33299e",
   "metadata": {},
   "source": [
    "On peut noter qu'un artiste  à plus de 800 oeuvres ( Vincent Van Gogh avec 877 oeuvres) et un artiste a 24 oeuvres (Jackson Pollock)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ba05f1-449d-4e46-b59c-9bc9c291dcfc",
   "metadata": {},
   "source": [
    "Nombre total d'oeuvres par genre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16bde0-5dc2-49df-8b21-e6d9224ce1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_expanded = artists['genre'].str.split(',').explode()\n",
    "expanded_data = artists.loc[genres_expanded.index, ['paintings']].assign(genre=genres_expanded.values)\n",
    "genre_paintings = expanded_data.groupby('genre')['paintings'].sum()\n",
    "\n",
    "genre_paintings.plot(kind='bar', color='lightblue', edgecolor='black')\n",
    "plt.title(\"Nombre total d'œuvres par genre\")\n",
    "plt.xlabel(\"Genre de peinture\")\n",
    "plt.ylabel(\"Nombre total d'œuvres\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "genre_paintings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06299502-077d-497b-9a55-98d43d1e1003",
   "metadata": {},
   "source": [
    "En observant le nombre d'artiste par genre, on remarque que J. Pollock est le seul artiste dans sa catégorie, tandis que la catégorie de V. Van Gogh compte le plus d'artiste. De plus, en observant le nombre d'oeuvre par genre, on observe que le post impressionim compte beaucoup plus d'oeuvres que l'Expressionism. Maintenant que la distrbution des genre et des artistes est bien comprises, nous allons nous mener une analyse des périodes de vie des artistes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb70e13-2d20-46dc-ae65-a3b8bd0d5aa3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Analyse de la période de vie des artistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670ea44c-8fa1-40e1-9495-ef341d6baaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists[['birth_year', 'death_year']] = artists['years'].str.split('-', expand=True)\n",
    "\n",
    "artists['birth_year'] = pd.to_numeric(artists['birth_year'], errors='coerce')\n",
    "artists['death_year'] = pd.to_numeric(artists['death_year'], errors='coerce')\n",
    "\n",
    "artists['birth_year'].dropna().plot(kind='hist', bins=20, color='salmon', edgecolor='black')\n",
    "plt.title(\"Distribution des années de naissance des artistes\")\n",
    "plt.xlabel(\"Année de naissance\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.show()\n",
    "\n",
    "artists['death_year'].dropna().plot(kind='hist', bins=20, color='purple', edgecolor='black')\n",
    "plt.title(\"Distribution des années de décès des artistes\")\n",
    "plt.xlabel(\"Année de décès\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad04183-a3e5-4f81-ab15-a16eff38aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "artists['life_duration'] = artists['death_year'] - artists['birth_year']\n",
    "artists['life_duration'] = artists['life_duration'].dropna()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "artists['life_duration'].plot(kind='hist', bins=20, color='purple', edgecolor='black')\n",
    "plt.title('Distribution de la durée de vie des artistes')\n",
    "plt.xlabel('Durée de vie (en années)')\n",
    "plt.ylabel('Nombre d\\'artistes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad73b6-b552-43df-9513-ef57ac6f784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_lived_artist = artists.loc[artists['life_duration'].idxmax()]\n",
    "longest_lived_name = longest_lived_artist['name']\n",
    "longest_lived_paintings = longest_lived_artist['paintings']\n",
    "longest_lived_duration = longest_lived_artist['life_duration']\n",
    "\n",
    "shortest_lived_artist = artists.loc[artists['life_duration'].idxmin()]\n",
    "shortest_lived_name = shortest_lived_artist['name']\n",
    "shortest_lived_paintings = shortest_lived_artist['paintings']\n",
    "shortest_lived_duration = shortest_lived_artist['life_duration']\n",
    "\n",
    "print(f\"Artiste avec la plus grande durée de vie : {longest_lived_name} ({longest_lived_paintings} œuvres, {longest_lived_duration} ans)\")\n",
    "print(f\"Artiste avec la durée de vie la plus courte : {shortest_lived_name} ({shortest_lived_paintings} œuvres, {shortest_lived_duration} ans)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439bc3d2-249a-47d4-b633-0b91b34b7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = artists['life_duration'].corr(artists['paintings'])\n",
    "print(f\"Corrélation entre durée de vie et nombre d’œuvres : {correlation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465cec2-572b-42b5-a925-49a17a7ee876",
   "metadata": {},
   "source": [
    "Nous observons que la majorité des artistes ont émergé à partir de 1800. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8840458a-95ca-4d83-bfbc-448b02adbe76",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Nationalités des artistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7133f96d-8e1a-4ea2-b86f-2fb324777253",
   "metadata": {},
   "outputs": [],
   "source": [
    "nationality_counts = artists['nationality'].value_counts()\n",
    "print(\"Nombre d'artistes par nationalité :\")\n",
    "print(nationality_counts)\n",
    "\n",
    "top_nationalities = nationality_counts.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_nationalities.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Les 10 nationalités les plus représentées parmi les artistes\")\n",
    "plt.xlabel(\"Nationalité\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "most_represented_nationality = nationality_counts.idxmax()\n",
    "least_represented_nationality = nationality_counts.idxmin()\n",
    "\n",
    "print(f\"Nationalité la plus représentée : {most_represented_nationality} ({nationality_counts.max()} artistes)\")\n",
    "print(f\"Nationalité la moins représentée : {least_represented_nationality} ({nationality_counts.min()} artiste(s))\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a1cbf8-0dfb-4b58-b40a-8b139b0583d2",
   "metadata": {},
   "source": [
    "On remarque que certains peintres ont plusieurs nationalités:\n",
    "- Spanish,Greek\n",
    "- French,British\n",
    "- French,Jewish,Belarusian\n",
    "- German,Swiss\n",
    "\n",
    "De ce fait, une personne étant en française et anglaise, sera comptée dans une catégorie distincte, ici \"French,British\". Dans l'optique de corriger cela, nous avons dans la cellule suivante compté les personnes ayant des nationalités différentes, dans les catégories associées à chacune de leurs nationalités. Une personne française et anglaise sera donc compté dans la catégorie française ainsi que la catégorie anglaise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d616fd7f-8e95-48bf-9538-a56e782bde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_nationalities = artists['nationality'].str.split(',').explode()\n",
    "nationality_counts = expanded_nationalities.value_counts()\n",
    "\n",
    "print(\"Nombre d'artistes par nationalité :\")\n",
    "print(nationality_counts)\n",
    "\n",
    "top_nationalities = nationality_counts.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_nationalities.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Les 10 nationalités les plus représentées parmi les artistes\")\n",
    "plt.xlabel(\"Nationalité\")\n",
    "plt.ylabel(\"Nombre d'artistes\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "most_represented_nationality = nationality_counts.idxmax()\n",
    "least_represented_count = nationality_counts.min()\n",
    "least_represented_nationalities = nationality_counts[nationality_counts == least_represented_count].index.tolist()\n",
    "\n",
    "\n",
    "print(f\"Nationalité la plus représentée : {most_represented_nationality} ({nationality_counts.max()} artistes)\")\n",
    "print(f\"Nationalité les moins représentées : {least_represented_nationalities} ({nationality_counts.min()} artiste(s))\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53d307-4e3f-41c8-a5cd-9d77ee7b1b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_paintings = artists.groupby('nationality')['paintings'].sum().sort_values(ascending=False)\n",
    "\n",
    "print(\"Nombre d'œuvres par pays :\")\n",
    "print(country_paintings)\n",
    "\n",
    "top_countries = country_paintings.head(10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_countries.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title(\"Les 10 pays avec le plus d'œuvres d'art\")\n",
    "plt.xlabel(\"Pays\")\n",
    "plt.ylabel(\"Nombre d'œuvres\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "most_paintings_country = country_paintings.idxmax()\n",
    "least_paintings_country = country_paintings.idxmin()\n",
    "\n",
    "print(f\"Pays avec le plus d'œuvres : {most_paintings_country} ({country_paintings.max()} œuvres)\")\n",
    "print(f\"Pays avec le moins d'œuvres : {least_paintings_country} ({country_paintings.min()} œuvre(s))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c551c9fa-e514-4113-a815-b05ff4dadafc",
   "metadata": {},
   "source": [
    "Nous observons que la majorité des artistes sont français ou italien, tandis que les artistes les moins nombeux sont orginaires de Belgique, d'Autriche, ...\n",
    "En outre, nous observons que la France est le pays contenant le plus d'oeuvres, suivi de l'Allemagne. L'Italie arrive en 4ème position, alors qu'elle compte deux fois plus d'artistes que l'Allemagne (4 artistes). Cela nous montre qu'un nombre plus élevé d'artiste dans un pays n'implique pas forcément un plus grand nombre d'oeuvres produites par ces artistes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d270da1-f43a-4d0d-8980-bee192c60c91",
   "metadata": {},
   "source": [
    "## Global params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d772421a-0033-4fea-afb1-ec9b1485b541",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "CHANNELS = 3\n",
    "\n",
    "IMAGE_HQ_SIZE = 224\n",
    "IMAGE_LQ_SIZE = 64\n",
    "MEAN_HQ = [0.485, 0.456, 0.406]\n",
    "STD_HQ = [0.229, 0.224, 0.225]\n",
    "TRANSFORM_HQ = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=transforms.InterpolationMode.BICUBIC),  # Interpolate\n",
    "    #transforms.Pad((32, 32, 32, 32)),  # Padding\n",
    "    transforms.CenterCrop(224),  # Center\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=MEAN_HQ, std=STD_HQ)\n",
    "])\n",
    "\n",
    "def load_image_as_rgb_matrices(image_name):    \n",
    "    img = Image.open(images_lq_path + image_name)\n",
    "    img = img.resize((IMAGE_LQ_SIZE, IMAGE_LQ_SIZE))  \n",
    "    img = img.convert('RGB')\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b393fb8-32c9-4dca-abb3-e0c0cf085036",
   "metadata": {},
   "source": [
    "## Creating LQ images loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193f3ec9-3d7d-42f1-bbfb-435134462f01",
   "metadata": {},
   "source": [
    "Dans cette partie, nous allons nous intéréesser au dossier `images_lq` contenant toutes les peintures, classées par artistes, en basse résolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5d4758-2c83-456d-b6ee-547e0304656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_lq_path = data_path + \"images_lq/\"\n",
    "images_filename = os.listdir(images_lq_path)\n",
    "\n",
    "list_name = []\n",
    "\n",
    "for filename in images_filename:\n",
    "    parts = filename.split('_')\n",
    "    if len(parts) > 3:\n",
    "        name_surname = f\"{parts[0]} {parts[1]} {parts[2]}\"\n",
    "    elif len(parts) == 3:\n",
    "        name_surname = f\"{parts[0]} {parts[1]}\"\n",
    "    elif len(parts) == 2:  # \"Name_xxx\" format\n",
    "        name_surname = parts[0]  # Only Name\n",
    "    list_name.append(name_surname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad38202-a508-4cc7-bd1c-66156ed6cc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(images_filename) == len(list_name):\n",
    "    total_lq_df = pd.DataFrame({\n",
    "        'filename': images_filename,\n",
    "        'artist': list_name,\n",
    "    })\n",
    "    print(\"DataFrame created successfully!\")\n",
    "else:\n",
    "    print(\"Error: The lists have different lengths.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3668c-9980-478e-8466-edc27bea59a7",
   "metadata": {},
   "source": [
    "### Dataloaders avec tous les artistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e43e66-a33c-440b-893d-7fec8e8d1711",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_LQ_SIZE = 64\n",
    "DATASET_LQ_SIZE = len(images_filename)\n",
    "CHANNELS = 3\n",
    "\n",
    "x = np.zeros((DATASET_LQ_SIZE, IMAGE_LQ_SIZE, IMAGE_LQ_SIZE, CHANNELS))\n",
    "for i in range(DATASET_LQ_SIZE):\n",
    "    x[i] = load_image_as_rgb_matrices(images_filename[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b5448f-1bb1-400c-84b8-1bd28cc45559",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.array(list(set(total_lq_df['artist'])))\n",
    "classes_as_int = np.array([np.where(classes == artist)[0][0] for artist in total_lq_df['artist']])\n",
    "\n",
    "x_train, x_test_val, y_train, y_test_val = train_test_split(\n",
    "    np.transpose(x, (0, 3, 1, 2)),  classes_as_int, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "x_test, x_val, y_test, y_val = train_test_split(\n",
    "    x_test_val, y_test_val, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize images\n",
    "x_train, x_val, x_test = x_train / 255.0, x_val / 255.0, x_test / 255.0\n",
    "\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset_lq = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset_lq = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "test_dataset_lq = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader_lq = DataLoader(train_dataset_lq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_lq = DataLoader(val_dataset_lq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_lq = DataLoader(test_dataset_lq, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset_lq)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_lq)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset_lq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1e4f37-d82b-4b62-ade9-83c059965eb2",
   "metadata": {},
   "source": [
    "### Dataloaders avec les artistes ayant plus de 85 oeuvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e6b241-851a-436b-8450-5e1015e66b24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "artist_counts_lq = Counter(total_lq_df['artist'])\n",
    "\n",
    "print(\"Nombre d'artistes :\", len(artist_counts_lq))\n",
    "\n",
    "print(\"\\nNombre d'oeuvres pour chaque artiste du dataset LQ :\")\n",
    "for artist, count in artist_counts_lq.items():\n",
    "    print(f\"{artist}: {count} oeuvres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1514c1f-4ea9-461f-8958-5d86d5719bd2",
   "metadata": {},
   "source": [
    "On remarque qu'un certain nombre d'artistes ont réalisé peu d'oeuvres. Dans la partie suivante, on décide de supprimer du dataframe les artistes avec moins de 85 oeuvres réaliser pour éviter qu'ils aient un impact négatif sur l'entrainement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aff7201-abb7-42dd-b4ca-5fb914f1d419",
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_counts_lq = Counter(total_lq_df['artist'])\n",
    "\n",
    "artists_with_more_than_85_lq = {\n",
    "    artist: count for artist, count in artist_counts_lq.items() if count > 85\n",
    "}\n",
    "\n",
    "artists_list_with_more_than_85_lq = list(artists_with_more_than_85_lq.keys())\n",
    "\n",
    "print(\"Nombre d'artistes avec plus de 85 oeuvres :\", len(artists_with_more_than_85_lq))\n",
    "\n",
    "print(\"\\nArtistes avec plus de 85 oeuvres dans les images LQ :\")\n",
    "for artist, count in artists_with_more_than_85_lq.items():\n",
    "    print(f\"{artist}: {count} oeuvres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ce3135-18ec-4e70-834d-e504aa609b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lq_filtered = total_lq_df[total_lq_df['artist'].isin(artists_list_with_more_than_85_lq)].copy()\n",
    "\n",
    "images_filename_filtered = total_lq_filtered['filename'].tolist()\n",
    "\n",
    "DATASET_LQ_SIZE = len(images_filename_filtered)\n",
    "print(f\"Nombre total d'images après filtrage : {DATASET_LQ_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eafe4e-4eb6-4791-870a-8ab75f4ebb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_filtered = np.zeros((DATASET_LQ_SIZE, IMAGE_LQ_SIZE, IMAGE_LQ_SIZE, CHANNELS))\n",
    "\n",
    "for i, filename in enumerate(images_filename_filtered):\n",
    "    x_filtered[i] = load_image_as_rgb_matrices(filename)\n",
    "    \n",
    "classes_filtered = np.array(list(set(total_lq_filtered['artist'])))\n",
    "classes_as_int_filtered = np.array([\n",
    "    np.where(classes_filtered == artist)[0][0] for artist in total_lq_filtered['artist']\n",
    "])\n",
    "\n",
    "x_train_filtered, x_test_val_filtered, y_train_filtered, y_test_val_filtered = train_test_split(\n",
    "    np.transpose(x_filtered, (0, 3, 1, 2)),  \n",
    "    classes_as_int_filtered, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "x_test_filtered, x_val_filtered, y_test_filtered, y_val_filtered = train_test_split(\n",
    "    x_test_val_filtered, \n",
    "    y_test_val_filtered, \n",
    "    test_size=0.5, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "x_train_filtered, x_val_filtered, x_test_filtered = x_train_filtered / 255.0, x_val_filtered / 255.0, x_test_filtered / 255.0\n",
    "\n",
    "x_train_tensor_filtered = torch.tensor(x_train_filtered, dtype=torch.float32)\n",
    "y_train_tensor_filtered = torch.tensor(y_train_filtered, dtype=torch.long)\n",
    "\n",
    "x_val_tensor_filtered = torch.tensor(x_val_filtered, dtype=torch.float32)\n",
    "y_val_tensor_filtered = torch.tensor(y_val_filtered, dtype=torch.long)\n",
    "\n",
    "x_test_tensor_filtered = torch.tensor(x_test_filtered, dtype=torch.float32)\n",
    "y_test_tensor_filtered = torch.tensor(y_test_filtered, dtype=torch.long)\n",
    "\n",
    "train_dataset_lq_filtered = TensorDataset(x_train_tensor_filtered, y_train_tensor_filtered)\n",
    "val_dataset_lq_filtered = TensorDataset(x_val_tensor_filtered, y_val_tensor_filtered)\n",
    "test_dataset_lq_filtered = TensorDataset(x_test_tensor_filtered, y_test_tensor_filtered)\n",
    "\n",
    "train_loader_lq_filtered = DataLoader(train_dataset_lq_filtered, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_lq_filtered = DataLoader(val_dataset_lq_filtered, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_lq_filtered = DataLoader(test_dataset_lq_filtered, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset_lq_filtered)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_lq_filtered)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset_lq_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d99900-436c-4073-8465-44415ce5da08",
   "metadata": {},
   "source": [
    "### Plots de quelques oeuvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc6267d-b663-44d1-be23-b105ce254a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_plots = 3\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(number_of_plots * 2, 2))\n",
    "\n",
    "for i in range(number_of_plots):\n",
    "    axes[i].imshow(x_filtered[i*100]/255.0)\n",
    "    axes[i].axis('off')\n",
    "    axes[i].set_title(f\"{list_name[i*100]}\")\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb90d3-75a1-4331-adb6-747dc749db05",
   "metadata": {},
   "source": [
    "## Creating HQ images loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf04bb3-e67f-4236-b9ec-db79f6586545",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc = \"./art-challenge/images_hq\"\n",
    "dataset_hq = datasets.ImageFolder(root=gc, transform=TRANSFORM_HQ)\n",
    "DATASET_HQ_SIZE = len(dataset_hq)\n",
    "CLASS_TO_IDX_HQ = dataset_hq.class_to_idx\n",
    "IDX_TO_CLASS_HQ = {idx: name for name, idx in CLASS_TO_IDX_HQ.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21684d77-a327-41cd-a10e-7f7f16155f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_TO_IDX_HQ = {key.replace('_', ' '): value for key, value in CLASS_TO_IDX_HQ.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f70c67-2dd5-40cb-be71-854d79ca9bb6",
   "metadata": {},
   "source": [
    "### Dataloaders avec tous les artistes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7759d7-d4fa-4fff-8fa7-2fd08bae6c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * DATASET_HQ_SIZE)\n",
    "val_size = int(0.1 * DATASET_HQ_SIZE)\n",
    "test_size = DATASET_HQ_SIZE - train_size - val_size\n",
    "\n",
    "train_dataset_hq, val_dataset_hq, test_dataset_hq = random_split(dataset_hq, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader_hq = DataLoader(train_dataset_hq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_hq = DataLoader(val_dataset_hq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_hq = DataLoader(test_dataset_hq, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset_hq)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_hq)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset_hq)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8719275-6732-47f3-a7e4-900fd90ea845",
   "metadata": {},
   "source": [
    "On remarque une différence du nombre d'oeuvres entre les dataloaders HQ et LQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc479b8d-cd93-4d67-8d0d-5800972a69e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_artists_hq = list(CLASS_TO_IDX_HQ.keys())\n",
    "\n",
    "print(\"Nombre d'artistes :\", len(list_of_artists_hq))\n",
    "\n",
    "print(\"\\nListe des artistes dans le dataset HQ:\")\n",
    "for artist in list_of_artists_hq:\n",
    "    print(artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14c5cc-d643-404c-b7d9-18c531c0f9a5",
   "metadata": {},
   "source": [
    "Le nombre d'artistes est identiques entre LQ et HQ. La différence en nombre d'oeuvres ne vient donc pas de là. Nous allons nous interessé au nombre d'oeuvres par artistes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f50ac-d4d8-49fd-b8f3-6cb6ea1d0547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre d'œuvres pour chaque artiste dans le dataset HQ :\")\n",
    "for artist in list_of_artists_hq:\n",
    "    if artist in artists['name'].values:\n",
    "        num_paintings = artists.loc[artists['name'] == artist, 'paintings'].values[0]\n",
    "        print(f\"{artist}: {num_paintings} œuvres\")\n",
    "    else:\n",
    "        print(f\"{artist}: Nombre d'œuvres inconnu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eeba68-10ec-4a06-9d07-ee3abda4c593",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Artiste(s) n'ayant pas le même nombre d'oeuvres LQ et HQ :\")\n",
    "for i in range(len(list_of_artists_hq)):\n",
    "    count_hq = artists.loc[artists['name'] == list_of_artists_hq[i], 'paintings'].values[0]\n",
    "    count_lq = artist_counts_lq.get(list_of_artists_hq[i])\n",
    "    if (count_hq - count_lq !=0):\n",
    "        print(list_of_artists_hq[i],\": Image LQ =\",count_lq,\",Image HQ =\", count_hq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d54090-9219-4df5-a969-2d4e888ece1d",
   "metadata": {},
   "source": [
    "### Dataloaders avec les artistes ayant plus de 85 oeuvres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad6da8-99ac-4c3a-88c0-ce04d199ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Long runtime ...\n",
    "CLASS_TO_IDX_HQ = dataset_hq.class_to_idx\n",
    "artist_counts = Counter()\n",
    "for _, label in dataset_hq:\n",
    "    artist = IDX_TO_CLASS_HQ[label]\n",
    "    artist_counts[artist] += 1\n",
    "\n",
    "artists_with_more_than_85_hq = {artist: count for artist, count in artist_counts.items() if count > 85}\n",
    "print(\"Artistes avec plus de 85 occurrences :\")\n",
    "for artist, count in artists_with_more_than_85_hq.items():\n",
    "    print(f\"{artist}: {count} occurrences\")\n",
    "\n",
    "hq_class_to_new_idx = {artist: idx for idx, artist in enumerate(artists_with_more_than_85_hq)}\n",
    "hq_idx_to_class = {idx: artist for artist, idx in hq_class_to_new_idx.items()}\n",
    "\n",
    "hq_indices = [CLASS_TO_IDX_HQ[artist] for artist in artists_with_more_than_85_hq if artist in CLASS_TO_IDX_HQ]\n",
    "\n",
    "filtered_hq_dataset = [data for data in dataset_hq if data[1] in hq_indices]\n",
    "\n",
    "filtered_hq_dataset = [(data[0], hq_class_to_new_idx[IDX_TO_CLASS_HQ[data[1]]]) for data in filtered_hq_dataset]\n",
    "\n",
    "train_size = int(0.8 * len(filtered_hq_dataset))\n",
    "val_size = int(0.1 * len(filtered_hq_dataset))\n",
    "test_size = len(filtered_hq_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset_hq_filtered, val_dataset_hq_filtered, test_dataset_hq_filtered = random_split(\n",
    "    filtered_hq_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "train_loader_hq_filtered = DataLoader(train_dataset_hq_filtered, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_hq_filtered = DataLoader(val_dataset_hq_filtered, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_hq_filtered = DataLoader(test_dataset_hq_filtered, batch_size=1, shuffle=False)\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset_hq_filtered)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_hq_filtered)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset_hq_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410d012-efa5-4b2a-867c-38ea281edae6",
   "metadata": {},
   "source": [
    "## Training et optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a9e63-f242-4f2f-af58-059f87721b65",
   "metadata": {},
   "source": [
    "### Fonctions `train` et `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea294fc3-3b40-4a7b-9eda-cd58356ce1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs=EPOCHS, save_model=False, model_path=\"cnn_weights.pth\"):\n",
    "    # Training of the model\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # FB\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # BW\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Do not need to apply softmax manually -> this order will be the same\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            correct_train += (preds == labels).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total_train_samples += images.size(0)\n",
    "        \n",
    "        train_loss = train_loss / total_train_samples\n",
    "        train_accuracy = correct_train / total_train_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                total_val_samples += images.size(0)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / total_val_samples\n",
    "        val_accuracy = correct_val / total_val_samples\n",
    "        \n",
    "        # Train accuracy could be used to check if the network learns something\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eea676-7d45-4b3c-b1f1-2b705f3cfd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model_path=\"cnn_weights.pth\"):\n",
    "    model = get_model(weights=None)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=DEVICE))\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    total_test_samples = 0\n",
    "    correct_test = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            test_loss += loss.item()\n",
    "            total_test_samples += images.size(0)\n",
    "                \n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_test += (preds == labels).sum().item()\n",
    "        \n",
    "        test_loss = test_loss / total_test_samples\n",
    "        test_accuracy = correct_test / total_test_samples\n",
    "\n",
    "        print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "    return test_loss, test_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d7e9d1-33ed-4965-9c91-3c371a4839bd",
   "metadata": {},
   "source": [
    "### ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cfad32-965b-42eb-b08f-a72f837ff3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(artists_with_more_than_85_hq)\n",
    "DROPOUT_RATE = 0.4\n",
    "\n",
    "def get_model(name='resnet18', weights='IMAGENET1K_V1', verbose=False):\n",
    "    model = getattr(models, name)(weights=weights)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=DROPOUT_RATE),                  \n",
    "        nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    ) \n",
    "\n",
    "    if verbose:\n",
    "        trainable_params =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'Trainable params: {trainable_params}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5b8d00-62b1-4c02-96bf-92846964834d",
   "metadata": {},
   "source": [
    "### Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a76de9-40f6-46d8-bb68-0e2e58fac067",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use Optuna to optimize the best hyper-params\n",
    "# Long time execution...\n",
    "# See https://github.com/optuna/optuna-examples/blob/main/pytorch/pytorch_simple.py\n",
    "def objective(trial):\n",
    "    # Define the model\n",
    "    model = get_model()\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # The different parameters we're trying to improve\n",
    "    fc_lr = trial.suggest_float(\"fc_lr\", 1e-4, 1e-2, log=True)  \n",
    "    base_lr = trial.suggest_float(\"base_lr\", 1e-6, 1e-4, log=True)\n",
    "\n",
    "    base_params = [p for name, p in model.named_parameters() if \"fc\" not in name]    \n",
    "    # Init optimizer\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': model.fc.parameters(), 'lr': fc_lr},      # LR for fully connected layer\n",
    "        {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "    ])\n",
    "\n",
    "    # Loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        model.train()\n",
    "        for images, labels in tqdm(train_loader_hq_filtered):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "                \n",
    "            # FB\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "                \n",
    "            # BW\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        total_val_samples = 0\n",
    "        correct_val = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader_hq_filtered):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                    \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                    \n",
    "                total_val_samples += images.size(0)\n",
    "                    \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "            \n",
    "        val_accuracy = correct_val / total_val_samples\n",
    "        trial.report(val_accuracy, epoch)\n",
    "    \n",
    "    # Handle pruning based on the intermediate value.\n",
    "    if trial.should_prune():\n",
    "        raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a8079-2726-46c8-840b-fada1e6a40b2",
   "metadata": {},
   "source": [
    "### Optimisation sur ResNet18 avec Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc60464-68a1-4dc9-a094-45d6c7790b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default sampler used: https://optuna.readthedocs.io/en/stable/reference/samplers/generated/optuna.samplers.TPESampler.html\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d5396-d434-406b-91a5-1577a8141ead",
   "metadata": {},
   "source": [
    "### Training avec les paramètres optimisés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda5d3c-eb73-4a8a-bdf2-650115dc2e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model1 = get_model(weights=None)\n",
    "model1 = model1.to(DEVICE)\n",
    "\n",
    "# Valeurs trouvées précédemment\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "\n",
    "base_params = [p for name, p in model1.named_parameters() if \"fc\" not in name] \n",
    "\n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model1.fc.parameters(), 'lr': fc_lr},      # LR for fully connected layer\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "\n",
    "train(model1, criterion, optimizer, train_loader_hq_filtered, val_loader_hq_filtered, epochs=10, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc56fdec-1e66-4890-8914-0d4c9ad02c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = test(test_loader_hq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ad5a4f-ec7b-42d7-9387-cce7c82207fb",
   "metadata": {},
   "source": [
    "On constate une convergence des accuracy d'entrainement et de validation dès la 5e itération. Le modèle a toutefois tendance à overfitter, avec une `train accuracy` de 0.99 mais une `val accuracy` = 0.79 et une `test accuracy` = 0.76."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f01f3f-bda4-4453-b371-5eaed6b5d477",
   "metadata": {},
   "source": [
    "### EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4a6a1f-8693-4c7a-b450-58b616eff7ea",
   "metadata": {},
   "source": [
    "EfficientNet existe en plusieurs versions:\n",
    "- EfficientNet_Bx (x $ \\in [0:7]$) : Précision, durée de calcul, profondeur de réseau et résolution augmentent avec x\n",
    "- EfficientNet_V2_t (t $ \\in $ [s,l,m]) : Version améliorée de EfficientNet_Bx, existe en 3 versions : Small, Medium et Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f0099b-2cc4-40e9-bc38-1de57cda19fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction get_model est redéfinie car EfficientNet ne possède pas de fully connected layer (.fc pour ResNet)\n",
    "def get_model_v2(name='efficientnet_v2_s', weights='IMAGENET1K_V1', verbose=False):\n",
    "    model = getattr(models, name)(weights=weights)\n",
    "    in_features = model.classifier[1].in_features\n",
    "    model.classifier[1] = nn.Linear(in_features, NUM_CLASSES)\n",
    "\n",
    "    if verbose:\n",
    "        trainable_params =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'Trainable params: {trainable_params}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932c092-f009-4158-8995-a9e474e3e84b",
   "metadata": {},
   "source": [
    "#### Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8a0bd-35db-4a89-9eb3-c4c7a63ebe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model_v2(name='efficientnet_v2_s')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "# Assure-toi que NUM_CLASSES est mis à jour avec les artistes filtrés\n",
    "NUM_CLASSES = len(artists_with_more_than_85)  # Nombre de classes après filtrage\n",
    "\n",
    "\n",
    "base_params = [p for name, p in model.named_parameters() if \"classifier.1\" not in name]    \n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.classifier[1].parameters(), 'lr': fc_lr},      # LR for fully connected layer\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#criterion = FocalLoss(alpha=alpha.to(DEVICE), gamma=2, reduction='mean')\n",
    "\n",
    "train(model, criterion, optimizer, train_loader_hq, val_loader_hq, epochs=10, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc567576-8665-40e5-a793-8c5e78aa99ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = test(test_loader_hq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb43d916-0ab2-4ae6-a9e2-f30a8a492ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour libérer la VRAM après entrainement du modèle\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e0362c-213c-456e-ba13-d804327fb935",
   "metadata": {},
   "source": [
    "Comme pour ResNet18, EfficientNet Small a tendance à overffiter (`train accuracy` = 0.99 et `val accuracy` = 0.83). La `test accuracy` est légerement meilleure (0.81 vs. 0.76). La durée d'entrainement est toutefois ~4x plus longue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84092f22-11b1-4492-b034-a67c749f509f",
   "metadata": {},
   "source": [
    "#### Medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd2fde5-dcb9-4cdd-856a-10144e61ed4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(name='efficientnet_v2_m')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "# Assure-toi que NUM_CLASSES est mis à jour avec les artistes filtrés\n",
    "NUM_CLASSES = len(artists_with_more_than_85)  # Nombre de classes après filtrage\n",
    "\n",
    "\n",
    "base_params = [p for name, p in model.named_parameters() if \"classifier.1\" not in name]    \n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.classifier[1].parameters(), 'lr': fc_lr},      # LR for fully connected layer\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#criterion = FocalLoss(alpha=alpha.to(DEVICE), gamma=2, reduction='mean')\n",
    "\n",
    "train(model, criterion, optimizer, train_loader_hq, val_loader_hq, epochs=10, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0266d89-3b2e-4d10-b795-385636425197",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss, acc = test(test_loader_hq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da873917-3c27-4165-a988-dcca9142639d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour libérer la VRAM après entrainement du modèle\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf261c-4145-49a1-baac-ec0905573ad4",
   "metadata": {},
   "source": [
    "#### Large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6663b-9a93-46d2-93fa-f9e662b51bd0",
   "metadata": {},
   "source": [
    "Le modèle est large est trop grand pour s'éxécuter ou alors s'éxécute dans un temps déraisonable (dépend de la VRAM et RAM disponible). Il est implémenté dans les cellules suivante mais n'a pas été testé pour l'instant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05752e9-64a9-4c90-abbb-e6a3d231ce75",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model(name='efficientnet_v2_l')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "# Assure-toi que NUM_CLASSES est mis à jour avec les artistes filtrés\n",
    "NUM_CLASSES = len(artists_with_more_than_85)  # Nombre de classes après filtrage\n",
    "\n",
    "\n",
    "base_params = [p for name, p in model.named_parameters() if \"classifier.1\" not in name]    \n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.classifier[1].parameters(), 'lr': fc_lr},      # LR for fully connected layer\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#criterion = FocalLoss(alpha=alpha.to(DEVICE), gamma=2, reduction='mean')\n",
    "\n",
    "train(model, criterion, optimizer, train_loader_hq, val_loader_hq, epochs=5, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b19ad3-d859-4f1b-9519-35cf19c578b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = test(test_loader_hq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f149bea-1ec3-4c37-9d50-5dc84ad9b495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour libérer la VRAM après entrainement du modèle\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abd7af8-0a83-47c9-93c6-cdd9b68ab258",
   "metadata": {},
   "source": [
    "### Inception V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a50391-a519-42b2-9ef9-811834bd9614",
   "metadata": {},
   "source": [
    "Pour le modèle Inception, les images doivent être de taille minimale 299x299."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9d8e58-a4cc-4b6d-b94c-6332fc7c6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORM_HQ_IncV3 = transforms.Compose([\n",
    "    transforms.Resize((299, 299)),  # Resize all images to 299x299\n",
    "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
    "    transforms.Normalize(\n",
    "        mean=MEAN_HQ,  # Normalize with ImageNet means\n",
    "        std=STD_HQ    # Normalize with ImageNet std deviations\n",
    "    ),\n",
    "])\n",
    "\n",
    "# Charger le dataset HQ\n",
    "gc = \"./art-challenge/images_hq\"\n",
    "dataset_hq = datasets.ImageFolder(root=gc, transform=TRANSFORM_HQ_IncV3)\n",
    "DATASET_HQ_SIZE = len(dataset_hq)\n",
    "CLASS_TO_IDX_HQ = dataset_hq.class_to_idx\n",
    "IDX_TO_CLASS_HQ = {idx: name for name, idx in CLASS_TO_IDX_HQ.items()}\n",
    "\n",
    "# Comptage des occurrences pour chaque artiste\n",
    "artist_counts = Counter()\n",
    "for _, label in dataset_hq:\n",
    "    artist = IDX_TO_CLASS_HQ[label]\n",
    "    artist_counts[artist] += 1\n",
    "\n",
    "# Filtrer les artistes ayant plus de 85 occurrences\n",
    "artists_with_more_than_85 = {artist: count for artist, count in artist_counts.items() if count > 85}\n",
    "print(\"Artistes avec plus de 85 occurrences :\")\n",
    "for artist, count in artists_with_more_than_85.items():\n",
    "    print(f\"{artist}: {count} occurrences\")\n",
    "\n",
    "# Réindexer les classes en fonction des artistes filtrés\n",
    "hq_class_to_new_idx = {artist: idx for idx, artist in enumerate(artists_with_more_than_85)}\n",
    "hq_idx_to_class = {idx: artist for artist, idx in hq_class_to_new_idx.items()}\n",
    "\n",
    "# Filtrer les indices des artistes sélectionnés\n",
    "hq_indices = [CLASS_TO_IDX_HQ[artist] for artist in artists_with_more_than_85 if artist in CLASS_TO_IDX_HQ]\n",
    "\n",
    "# Filtrer les données du dataset HQ pour ne garder que les indices valides\n",
    "filtered_hq_dataset = [data for data in dataset_hq if data[1] in hq_indices]\n",
    "\n",
    "# Mise à jour des labels du dataset filtré pour correspondre aux nouveaux indices\n",
    "filtered_hq_dataset = [(data[0], hq_class_to_new_idx[IDX_TO_CLASS_HQ[data[1]]]) for data in filtered_hq_dataset]\n",
    "\n",
    "# Création des ensembles de données filtrés\n",
    "train_size = int(0.8 * len(filtered_hq_dataset))\n",
    "val_size = int(0.1 * len(filtered_hq_dataset))\n",
    "test_size = len(filtered_hq_dataset) - train_size - val_size\n",
    "\n",
    "train_dataset_hq, val_dataset_hq, test_dataset_hq = random_split(\n",
    "    filtered_hq_dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "# Chargement des ensembles de données dans les DataLoaders\n",
    "train_loader_hq = DataLoader(train_dataset_hq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_hq = DataLoader(val_dataset_hq, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_hq = DataLoader(test_dataset_hq, batch_size=1, shuffle=False)\n",
    "\n",
    "# Affichage des tailles des datasets\n",
    "print(f\"Train dataset size: {len(train_dataset_hq)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset_hq)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset_hq)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d39d3-3269-4cd2-8dd5-f8c978a181e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_CLASSES = len(dataset_hq.classes) # Same value for lQ dataset\n",
    "NUM_CLASSES = len(artists_with_more_than_85)\n",
    "DROPOUT_RATE = 0.4\n",
    "MODEL_PATH = \"cnn_weights.pth\"\n",
    "\n",
    "def get_model(name='inception_v3', weights='IMAGENET1K_V1', verbose=False):\n",
    "    model = getattr(models, name)(weights=weights)\n",
    "    model.fc = nn.Sequential(\n",
    "        nn.Dropout(p=DROPOUT_RATE),                  \n",
    "        nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "    ) \n",
    "\n",
    "    if verbose:\n",
    "        trainable_params =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'Trainable params: {trainable_params}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917823a8-1531-476c-9379-753b90c8f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, train_loader, val_loader, epochs=EPOCHS, save_model=False, model_path=MODEL_PATH):\n",
    "    # Training of the model\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        total_train_samples = 0\n",
    "        correct_train = 0\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # FB\n",
    "            main_logits, aux_logits = model(images)\n",
    "            loss1 = criterion(main_logits, labels)\n",
    "            loss2 = criterion(aux_logits, labels)\n",
    "            loss = loss1 + 0.4*loss2                    #0.4 has been chosen by the author of the original Inception paper\n",
    "            \n",
    "            # BW\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Do not need to apply softmax manually -> this order will be the same\n",
    "            _, preds = torch.max(main_logits, 1)\n",
    "            correct_train += (preds == labels).sum().item()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            total_train_samples += images.size(0)\n",
    "        \n",
    "        train_loss = train_loss / total_train_samples\n",
    "        train_accuracy = correct_train / total_train_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        total_val_samples = 0\n",
    "        correct_val = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader):\n",
    "                images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                total_val_samples += images.size(0)\n",
    "                \n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += (preds == labels).sum().item()\n",
    "        \n",
    "        val_loss = val_loss / total_val_samples\n",
    "        val_accuracy = correct_val / total_val_samples\n",
    "        \n",
    "        # Train accuracy could be used to check if the network learns something\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), model_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328da830-b76f-4a25-a699-25dda0514353",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(name='inception_v3', weights='IMAGENET1K_V1', verbose=False)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# The best values found previously\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "# Assure-toi que NUM_CLASSES est mis à jour avec les artistes filtrés\n",
    "NUM_CLASSES = len(artists_with_more_than_85)  # Nombre de classes après filtrage\n",
    "\n",
    "\n",
    "base_params = [p for name, p in model.named_parameters() if \"fc\" not in name]    \n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.fc.parameters(), 'lr': fc_lr},      # LR for fully connected layer\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#criterion = FocalLoss(alpha=alpha.to(DEVICE), gamma=2, reduction='mean')\n",
    "\n",
    "train(model, criterion, optimizer, train_loader_hq, val_loader_hq, epochs=10, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec9e399-e79a-46bc-bb01-27da7772afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = test(test_loader_hq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfb21bc-4b9f-4e87-aabd-79bec8274f30",
   "metadata": {},
   "source": [
    "### VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e757f3c7-4cc7-466e-873c-45bfad4c417d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NUM_CLASSES = len(dataset_hq.classes) # Same value for lQ dataset\n",
    "NUM_CLASSES = len(artists_with_more_than_85)\n",
    "DROPOUT_RATE = 0.4\n",
    "MODEL_PATH = \"cnn_weights.pth\"\n",
    "\n",
    "def get_model(name='vgg16', weights='IMAGENET1K_V1', verbose=False):\n",
    "    model = getattr(models, name)(weights=weights)\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Linear(model.classifier[0].in_features, 4096),  # Maintain VGG16's architecture\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=DROPOUT_RATE),\n",
    "        nn.Linear(4096, 4096),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(p=DROPOUT_RATE),\n",
    "        nn.Linear(4096, NUM_CLASSES)  # Output layer with NUM_CLASSES units\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        trainable_params =  sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f'Trainable params: {trainable_params}')\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1130feb3-4a0c-4650-9378-d9fbd36441a9",
   "metadata": {},
   "source": [
    "#### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacc0b60-e659-41e1-9553-19f57d19f19d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = get_model(name='vgg16')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# The best values found previously\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "# Assure-toi que NUM_CLASSES est mis à jour avec les artistes filtrés\n",
    "NUM_CLASSES = len(artists_with_more_than_85)  # Nombre de classes après filtrage\n",
    "\n",
    "base_params = [p for name, p in model.named_parameters() if \"classifier\" not in name]\n",
    "classifier_params = model.classifier.parameters() \n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': classifier_params, 'lr': fc_lr},          # LR for classifier layers\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#criterion = FocalLoss(alpha=alpha.to(DEVICE), gamma=2, reduction='mean')\n",
    "\n",
    "train(model, criterion, optimizer, train_loader_hq, val_loader_hq, epochs=5, save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad62c26-6d99-4ae0-ad40-21081d546668",
   "metadata": {},
   "source": [
    "#### VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5654142-a6ac-44cb-9aa5-4b9ffa2bfbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = get_model(name='vgg19')\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# The best values found previously\n",
    "fc_lr = 6.47e-4  \n",
    "base_lr = 8.587e-5\n",
    "# Assure-toi que NUM_CLASSES est mis à jour avec les artistes filtrés\n",
    "NUM_CLASSES = len(artists_with_more_than_85)  # Nombre de classes après filtrage\n",
    "\n",
    "base_params = [p for name, p in model.named_parameters() if \"classifier\" not in name]\n",
    "classifier_params = model.classifier.parameters() \n",
    "# Init optimizer\n",
    "optimizer = optim.Adam([\n",
    "    {'params': classifier_params, 'lr': fc_lr},          # LR for classifier layers\n",
    "    {'params': base_params, 'lr': base_lr}               # LR for pretrained layers\n",
    "])\n",
    "\n",
    "# Loss\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "#criterion = FocalLoss(alpha=alpha.to(DEVICE), gamma=2, reduction='mean')\n",
    "\n",
    "train(model, criterion, optimizer, train_loader_hq, val_loader_hq, epochs=5, save_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce7c6c0-4951-4bde-bf11-a4a5f7eecaf9",
   "metadata": {},
   "source": [
    "## Focal Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441dd14f-b8cb-40c6-83a2-be06ea812485",
   "metadata": {},
   "source": [
    "Comme observé durant l'analyse descriptive, les artistes n'ont pas tous le même nombre d'oeuvres. En effet, certains artistes ont une vingtaine d'oeuvres tandis que d'autres en ont plus de 800. Cela peut s'avérer être assez problématique dans le calcul de la loss, de ce fait, nous allons utiliser une focal loss. Cette loss permet de traiter les déséquilibre dans les \"classes\", elle est une modification de la cross entropy en ajoutant un terme de pondération. \n",
    "Cette perte permettra d'éviter que certains artistes avec beaucoup d'oeuvres contribuent trop à la loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f6ff75-c75e-451f-b1e5-d54fa9d9a18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=None, gamma=2, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha  # Poids par classe (peut être un tensor)\n",
    "        self.gamma = gamma  # Facteur de focalisation\n",
    "        self.reduction = reduction  # mean, sum ou none\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Appliquer un softmax pour convertir les logits en probabilités\n",
    "        probs = F.softmax(inputs, dim=1)\n",
    "        targets_one_hot = F.one_hot(targets, num_classes=inputs.size(1)).float()\n",
    "        \n",
    "        # Extraire les probabilités des classes cibles\n",
    "        pt = (probs * targets_one_hot).sum(dim=1)\n",
    "        \n",
    "        # Calculer la focal loss\n",
    "        focal_loss = -((1 - pt) ** self.gamma) * torch.log(pt)\n",
    "        \n",
    "        # Appliquer les poids de classe si fournis\n",
    "        if self.alpha is not None:\n",
    "            at = self.alpha.gather(0, targets)\n",
    "            focal_loss *= at\n",
    "        \n",
    "        # Réduire la perte si nécessaire\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22827c29-dc3c-4d47-93f1-a0a62efde137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les informations sur les artistes et leurs œuvres\n",
    "artist_counts = artists['paintings'].values\n",
    "total_paintings = artist_counts.sum()\n",
    "\n",
    "# Inverser la proportion pour accorder plus de poids aux artistes avec peu d'œuvres\n",
    "weights = total_paintings / (len(artist_counts) * artist_counts)\n",
    "alpha = torch.tensor(weights, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3fa8ee-1993-448e-b9f3-9f83060cc9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Fréquence des classes dans le dataset\n",
    "class_counts = Counter(dataset_hq.targets)  # `targets` contient les labels des artistes\n",
    "total_samples = sum(class_counts.values())\n",
    "\n",
    "# Calcul des poids inversés pour chaque classe\n",
    "class_weights = {cls: total_samples / count for cls, count in class_counts.items()}\n",
    "class_weights = torch.tensor([class_weights[cls] for cls in sorted(class_counts.keys())], dtype=torch.float32).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e634a-f89c-4161-a295-3c280f763be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLossWithWeights(nn.Module):\n",
    "    def __init__(self, alpha=1.0, gamma=2.0, class_weights=None):\n",
    "        super(FocalLossWithWeights, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights  # Poids spécifiques aux classes\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # Appliquer softmax pour obtenir les probabilités\n",
    "        probs = torch.softmax(inputs, dim=1)\n",
    "        probs = probs.gather(1, targets.view(-1, 1)).squeeze()\n",
    "\n",
    "        # Focal Weighting\n",
    "        focal_weight = (1 - probs) ** self.gamma\n",
    "\n",
    "        # Log-loss avec les poids des classes\n",
    "        if self.class_weights is not None:\n",
    "            weights = self.class_weights[targets]\n",
    "            loss = -self.alpha * weights * focal_weight * torch.log(probs + 1e-8)\n",
    "        else:\n",
    "            loss = -self.alpha * focal_weight * torch.log(probs + 1e-8) # +1e-8 pour éviter d'avoir 0\n",
    "\n",
    "        return loss.mean()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
