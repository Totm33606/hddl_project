{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "# Project CVAE on MNIST datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Fashion-MNIST datasets\n",
    "# Note: to improve speed we could save images as flatten images (using tranform for example)\n",
    "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Affichage aléatoire d'un exemple de chaque classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionnaire pour mapper les labels aux noms des classes\n",
    "class_names = [  \n",
    "    'class 0: T-shirt/top', 'class 1: Trouser', 'class 2: Pullover', 'class 3: Dress', 'class 4: Coat',\n",
    "    'class 5: Sandal', 'class 6: Shirt', 'class 7: Sneaker', 'class 8: Bag', 'class 9: Ankle boot'\n",
    "]\n",
    "\n",
    "def plot_random_samples_per_class(dataset):\n",
    "    # Initialiser un dictionnaire pour stocker un échantillon par classe\n",
    "    samples = {i: None for i in range(10)}\n",
    "\n",
    "    # Parcourir le dataset et sélectionner un échantillon aléatoire par classe\n",
    "    for img, label in torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True):\n",
    "        label = label.item()\n",
    "        if samples[label] is None:  # Sélectionner le premier échantillon trouvé pour chaque classe\n",
    "            samples[label] = img\n",
    "        if all(v is not None for v in samples.values()):  # Arrêter une fois qu'on a un échantillon de chaque classe\n",
    "            break\n",
    "\n",
    "    # Créer la figure avec des sous-graphiques 2x5\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 6))\n",
    "    fig.suptitle(\"Random Sample from Each FashionMNIST Class\")\n",
    "\n",
    "    # Afficher chaque image dans le subplot correspondant\n",
    "    for i, (label, img) in enumerate(samples.items()):\n",
    "        ax = axes[i // 5, i % 5]  # Position dans la grille\n",
    "        ax.imshow(img.squeeze(), cmap=\"gray\")\n",
    "        ax.set_title(class_names[label])\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])  # Ajuster l'espacement\n",
    "    plt.show()\n",
    "\n",
    "# Appeler la fonction pour afficher les échantillons\n",
    "plot_random_samples_per_class(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe 1 est différente des autres puisqu'il s'agit d'un pantalon (bas), aprés les classes.\n",
    "En s'intéressant au contenu des classes, nous observons qu'une classe contient des vetements/chaussures/accessoires assez différents (forme assez variable), la variance intra classe est assez grande, cela explique le fait que les individus d'une même classe peuvent être assez éloigné 5 ( cluster pas très compact). La variance inter classe n'est pas non plus très grande puisque certains articles peuvent être similaires à d'autres. Cela peut impliquer une superposition d'individus de différentes classes ( ou overlapping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation between VAE and Conditional VAE (CVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources :\n",
    "- https://proceedings.neurips.cc/paper/2015/file/8d55a249e6baa5c06772297520da2051-Paper.pdf\n",
    "- https://arxiv.org/pdf/1312.6114\n",
    "- https://lilianweng.github.io/posts/2018-08-12-vae/\n",
    "\n",
    "In addition of the input data, the CVAE takes a conditional variable (a class label in our case) in input of both encoder and decoder. The goal of a CVAE is to shape the latent shape to correspond to the condition variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global values\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "BATCH_SIZE = 64\n",
    "MAX_EPOCHS = 1\n",
    "LEARNING_RATE = 1e-3\n",
    "CLASSES_TO_IDX = train_dataset.class_to_idx\n",
    "IDX_TO_CLASSES = {idx: cls for cls, idx in CLASSES_TO_IDX.items()}\n",
    "FEATURE_SIZE = train_dataset.data[0].shape[0]\n",
    "CLASS_SIZE = 10\n",
    "LATENT_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CVAE implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first image of each dataset\n",
    "# Note: Do not run it after splitting datas into validation and test sets\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(10, 3))\n",
    "\n",
    "axs[0].imshow(train_dataset.data[100], cmap='gray')\n",
    "axs[0].set_title(f\"{list(CLASSES_TO_IDX.keys())[train_dataset.targets[100]]}\")\n",
    "\n",
    "axs[1].imshow(test_dataset.data[0], cmap='gray')\n",
    "axs[1].set_title(f\"{list(CLASSES_TO_IDX.keys())[test_dataset.targets[0]]}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split test between validation and test\n",
    "val_size = int(test_dataset.data.shape[0] / 2)\n",
    "test_size = test_dataset.data.shape[0] - val_size\n",
    "test_dataset, val_dataset = random_split(test_dataset, [test_size, val_size])\n",
    "\n",
    "# Some prints\n",
    "print(f\"Train dataset size: {len(train_dataset.data)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Makes dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model used : Conditional Variational Auto-Encoders (CVAE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://github.com/unnir/cVAE/blob/master/cvae.py\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, feature_size, latent_size, class_size):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.feature_size = feature_size\n",
    "        self.class_size = class_size\n",
    "\n",
    "        # encode\n",
    "        self.fc1  = nn.Linear(feature_size * feature_size + class_size, 400)\n",
    "        self.fc21 = nn.Linear(400, latent_size)\n",
    "        self.fc22 = nn.Linear(400, latent_size)\n",
    "\n",
    "        # decode\n",
    "        self.fc3 = nn.Linear(latent_size + class_size, 400)\n",
    "        self.fc4 = nn.Linear(400, feature_size * feature_size)\n",
    "\n",
    "        self.elu = nn.ELU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Center for each class in latent space\n",
    "        self.class_centers = nn.Parameter(torch.randn(class_size, latent_size))\n",
    "\n",
    "    def encode(self, x, c): # Q(z|x, c)\n",
    "        inputs = torch.cat([x, c], 1)\n",
    "        h1 = self.elu(self.fc1(inputs))\n",
    "        z_mu = self.fc21(h1)\n",
    "        z_var = self.fc22(h1)\n",
    "        return z_mu, z_var\n",
    "\n",
    "    def sample(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z, c): # P(x|z, c)\n",
    "        inputs = torch.cat([z, c], 1)\n",
    "        h3 = self.elu(self.fc3(inputs))\n",
    "        recon = self.sigmoid(self.fc4(h3))\n",
    "        return recon.view(inputs.size(0), 1, self.feature_size, self.feature_size)\n",
    "\n",
    "    def forward(self, x, c):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        mu, logvar = self.encode(x, c)\n",
    "        z = self.sample(mu, logvar)\n",
    "        return self.decode(z, c), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta=1):\n",
    "    recon_x = recon_x.view(recon_x.size(0), -1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + beta * KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette perte fonctionne en choisissant trois exemples :\n",
    "# Ancre : un point latent de la classe cible.\n",
    "# Positif : un autre point latent de la même classe.\n",
    "# Négatif : un point latent d'une classe différente.\n",
    "# La triplet loss pousse l’ancre et le positif à se rapprocher, et l’ancre et le négatif à s'éloigner. Cela peut être plus efficace pour séparer les classes.\n",
    "\n",
    "def triplet_loss(mu, labels, margin=1.0):\n",
    "    batch_size = mu.size(0)\n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        anchor = mu[i]\n",
    "        pos_indices = (labels == labels[i]).nonzero().view(-1)\n",
    "        neg_indices = (labels != labels[i]).nonzero().view(-1)\n",
    "        \n",
    "        if len(pos_indices) > 1 and len(neg_indices) > 0:\n",
    "            pos_index = pos_indices[torch.randint(1, len(pos_indices), (1,))].item()\n",
    "            neg_index = neg_indices[torch.randint(0, len(neg_indices), (1,))].item()\n",
    "            positive = mu[pos_index]\n",
    "            negative = mu[neg_index]\n",
    "            \n",
    "            # Triplet loss calculation\n",
    "            pos_dist = torch.sum((anchor - positive) ** 2)\n",
    "            neg_dist = torch.sum((anchor - negative) ** 2)\n",
    "            loss += torch.relu(pos_dist - neg_dist + margin)\n",
    "    loss /= batch_size\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_loss(recon_x, x, mu, logvar, labels, class_centers, beta=1, lambda_center=0.1, lambda_triplet=5):\n",
    "    # Reconstruction loss\n",
    "    recon_x = recon_x.view(recon_x.size(0), -1)\n",
    "    x = x.view(x.size(0), -1)\n",
    "    BCE = F.binary_cross_entropy(recon_x, x, reduction='sum')\n",
    "    \n",
    "    # KL Divergence\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    # Center loss\n",
    "    batch_size = mu.size(0)\n",
    "    center_loss = 0\n",
    "    for i in range(batch_size):\n",
    "        target_center = class_centers[labels[i].argmax()]  # get the center for the correct class\n",
    "        center_loss += torch.sum((mu[i] - target_center) ** 2)  # distance to the target center\n",
    "    center_loss /= batch_size\n",
    "    \n",
    "    # Triplet loss for better separation\n",
    "    triplet_loss_value = triplet_loss(mu, labels.argmax(dim=1))\n",
    "    \n",
    "    # Total loss with center loss and triplet loss\n",
    "    return BCE + beta * KLD + lambda_center * center_loss + lambda_triplet * triplet_loss_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Useful functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(labels, class_size):\n",
    "    targets = torch.zeros(labels.size(0), class_size)\n",
    "    for i, label in enumerate(labels):\n",
    "        targets[i, label] = 1\n",
    "    return targets.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr(optimizer):\n",
    "    return optimizer.param_groups[0]['lr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    loss_name,\n",
    "    device=DEVICE,\n",
    "    scheduler=None,\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    class_size=CLASS_SIZE,\n",
    "    save_model=False,\n",
    "    save_path=\"cvae_weights.pth\"\n",
    "):\n",
    "    model = model.to(device)\n",
    "    train_loss_l = torch.zeros(max_epochs)\n",
    "    val_loss_l = torch.zeros(max_epochs)\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # ** Training **\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        for data, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{max_epochs} Training\"):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if class_size is not None:\n",
    "                labels = one_hot(labels, class_size)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            if loss_name == \"recons\":\n",
    "                loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            else:\n",
    "                beta = min(0.01 , 0.2 )  # increase until target_beta\n",
    "                lambda_triplet = min(2+ epoch * 0.5, 8)  # target_lambda_triplet could be 15 or 20\n",
    "                loss = new_loss(recon_batch, data, mu, logvar, labels, model.class_centers, beta=beta, lambda_triplet=lambda_triplet)\n",
    "                \n",
    "            # Backward pass and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_train_loss += loss.item()\n",
    "        train_loss = running_train_loss / len(train_loader.dataset)\n",
    "\n",
    "        # ** Validation **\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data, labels in tqdm(val_loader, desc=f\"Epoch {epoch}/{max_epochs} Validation\"):\n",
    "                data, labels = data.to(device), labels.to(device)\n",
    "                if class_size is not None:\n",
    "                    labels = one_hot(labels, class_size)\n",
    "\n",
    "                # Forward pass\n",
    "                recon_batch, mu, logvar = model(data, labels)\n",
    "                loss = loss_function(recon_batch, data, mu, logvar)\n",
    "                running_val_loss += loss.item()\n",
    "        val_loss = running_val_loss / len(val_loader.dataset)\n",
    "\n",
    "        # Saving Losses\n",
    "        train_loss_l[epoch - 1] = train_loss\n",
    "        val_loss_l[epoch - 1] = val_loss\n",
    "\n",
    "        # Print losses\n",
    "        print(f\"Epoch {epoch}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Update LR\n",
    "        if scheduler:\n",
    "            print(f\"Learning Rate: {get_lr(optimizer):.6f}\")\n",
    "            scheduler.step()\n",
    "\n",
    "    # Save trained model\n",
    "    if save_model:\n",
    "        torch.save(model.state_dict(), save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "    return train_loss_l, val_loss_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE(FEATURE_SIZE, LATENT_SIZE, CLASS_SIZE)\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# Call the train function\n",
    "train_loss, val_loss = train_model(\n",
    "    model=cvae,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_name=\"recons\",\n",
    "    scheduler=scheduler,\n",
    "    save_model=True,\n",
    ")\n",
    "\n",
    "# Plot losses\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_loss, label=\"Train Loss\")\n",
    "plt.plot(val_loss, label=\"Validation Loss\")\n",
    "plt.title(\"Loss Curves\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    loss_name,\n",
    "    device=DEVICE,\n",
    "    class_size=CLASS_SIZE,\n",
    "    weights_path=\"cvae_weights.pth\",\n",
    "    plot_latent_space=True,\n",
    "    cmap=\"tab10\"\n",
    "):\n",
    "    # Load model\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    running_test_loss = 0.0\n",
    "    total_mse = 0.0\n",
    "    residuals_sum = 0\n",
    "\n",
    "    # Init plot\n",
    "    if plot_latent_space:\n",
    "        fig, ax = plt.subplots()\n",
    "        scatter = ax.scatter([], [], c=[], cmap=cmap, vmin=0, vmax=class_size - 1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, labels in tqdm(test_loader, desc=\"Testing...\"):\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if class_size is not None:\n",
    "                labels = one_hot(labels, class_size)\n",
    "\n",
    "            # Forward pass\n",
    "            recon_batch, mu, logvar = model(data, labels)\n",
    "            if loss_name == \"recons\":\n",
    "                loss = loss_function(recon_batch, data, mu, logvar)\n",
    "            else:\n",
    "                loss = new_loss(recon_batch, data, mu, logvar, labels, model.class_centers)\n",
    "\n",
    "            # Mean Squared Error\n",
    "            mse_value = F.mse_loss(recon_batch, data, reduction='sum')\n",
    "            total_mse += mse_value.item()\n",
    "            residuals_sum += torch.abs(data - recon_batch).sum(dim=0)\n",
    "            \n",
    "            running_test_loss += loss.item()\n",
    "\n",
    "            # Update plot\n",
    "            if plot_latent_space:\n",
    "                mu, labels = mu.cpu(), labels.cpu()\n",
    "                scatter = ax.scatter(\n",
    "                    mu[:, 0], mu[:, 1], c=labels.argmax(dim=1), cmap=cmap, vmin=0, vmax=class_size - 1\n",
    "                )\n",
    "\n",
    "    # Compute means\n",
    "    test_loss = running_test_loss / len(test_loader.dataset)\n",
    "    mse = total_mse / len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Test loss: {test_loss:.4f} -- MSE: {mse:.4f}\")\n",
    "\n",
    "    if plot_latent_space:\n",
    "        plt.colorbar(scatter)\n",
    "        plt.title(\"Latent Space\")\n",
    "        plt.show()\n",
    "\n",
    "    return test_loss, mse, residuals_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae = CVAE(FEATURE_SIZE, LATENT_SIZE, CLASS_SIZE)\n",
    "\n",
    "# Call the test function\n",
    "test_loss, mse, residuals_sum = test_model(\n",
    "    model=cvae,\n",
    "    test_loader=test_loader,\n",
    "    loss_name=\"recons\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes some plots\n",
    "def plot_test(\n",
    "    model,\n",
    "    test_loader,\n",
    "    device=DEVICE,\n",
    "    class_size=CLASS_SIZE,\n",
    "    classes=IDX_TO_CLASSES,\n",
    "    batch_size=1,\n",
    "    cmap=\"gray\",\n",
    "    figsize=(8, 8),\n",
    "    num_samples=2\n",
    "):\n",
    "    model.eval()\n",
    "    fig, axs = plt.subplots(nrows=num_samples, ncols=2, figsize=figsize)\n",
    "    loader_iter = iter(test_loader)\n",
    "\n",
    "    if num_samples > len(test_loader.dataset):\n",
    "        print(\"Reduce the batch-size or the num_samples value!\")\n",
    "        return\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row in range(num_samples):\n",
    "            data, labels = next(loader_iter)\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            if class_size is not None:\n",
    "                labels = one_hot(labels, class_size)\n",
    "\n",
    "            recon, _, _ = model(data, labels)\n",
    "\n",
    "            # Choose one sample in the batch\n",
    "            if batch_size > 1:\n",
    "                idx = random.randint(0, batch_size - 1)\n",
    "            else:\n",
    "                idx = 0\n",
    "\n",
    "            # Move to CPU for plot\n",
    "            original = data[idx].cpu().squeeze()\n",
    "            reconstructed = recon[idx].cpu().detach().squeeze()\n",
    "            label_idx = labels[idx].argmax() if class_size else labels[idx]\n",
    "            label_name = classes[label_idx.item()] if classes else f\"Class {label_idx}\"\n",
    "\n",
    "            # Plot\n",
    "            axs[row, 0].imshow(original, cmap=cmap)\n",
    "            axs[row, 0].set_title(f\"Original: {label_name}\")\n",
    "            axs[row, 0].axis(\"off\")\n",
    "\n",
    "            axs[row, 1].imshow(reconstructed, cmap=cmap)\n",
    "            axs[row, 1].set_title(\"Reconstructed\")\n",
    "            axs[row, 1].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_test(cvae, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "num_samples = 5  # Nombre d'échantillons par classe\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "cvae.to(DEVICE)\n",
    "\n",
    "# Générer des images pour chaque classe\n",
    "def generate_and_plot_samples(cvae_model, num_classes, num_samples, latent_size, feature_size):\n",
    "    fig, axes = plt.subplots(num_classes, num_samples, figsize=(15, 10))\n",
    "    fig.suptitle(\"Échantillons générés par classe\")\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        # Créer un vecteur one-hot pour la classe actuelle\n",
    "        class_condition = torch.zeros(num_classes, device=DEVICE)\n",
    "        class_condition[class_idx] = 1\n",
    "        class_condition = class_condition.unsqueeze(0).repeat(num_samples, 1)  # Répéter pour chaque échantillon\n",
    "\n",
    "        # Générer des échantillons aléatoires dans l'espace latent\n",
    "        z = torch.randn(num_samples, latent_size, device=DEVICE)\n",
    "\n",
    "        # Générer des images à partir de chaque échantillon latent\n",
    "        with torch.no_grad():\n",
    "            generated_images = cvae_model.decode(z, class_condition)\n",
    "\n",
    "        # Afficher chaque image générée dans un sous-graphe\n",
    "        for i in range(num_samples):\n",
    "            ax = axes[class_idx, i]\n",
    "            ax.imshow(generated_images[i].squeeze().cpu(), cmap=\"gray\")\n",
    "            ax.axis(\"off\")\n",
    "            if i == 0:\n",
    "                ax.set_ylabel(f\"Classe {class_idx}\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "generate_and_plot_samples(cvae, num_classes, num_samples, latent_size=LATENT_SIZE, feature_size=FEATURE_SIZE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
